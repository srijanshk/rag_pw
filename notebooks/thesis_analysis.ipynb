{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis Analysis Notebook\n",
    "\n",
    "This notebook analyzes results for RQ1–RQ4 and summarizes findings. It expects files in `rag_pw/results/{cot,static,dynamic}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & config\n",
    "from __future__ import annotations\n",
    "import json, os, math, glob, re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT = Path('rag_pw/results')\n",
    "COT_DIR    = ROOT / 'cot'\n",
    "STATIC_DIR = ROOT / 'static'\n",
    "DYN_DIR    = ROOT / 'dynamic'\n",
    "\n",
    "def latest(pattern: str) -> Optional[Path]:\n",
    "    files = sorted(Path('.').glob(pattern))\n",
    "    return files[-1] if files else None\n",
    "\n",
    "CFG = {\n",
    "  'cot': {\n",
    "    'gsm8k': latest(str(COT_DIR / 'gsm_COT_*.jsonl')),\n",
    "    'math':  latest(str(COT_DIR / 'math_COT_*.jsonl')),\n",
    "    'math500': latest(str(COT_DIR / 'math500_COT_*.jsonl')),\n",
    "  },\n",
    "  'static': {\n",
    "    'gsm8k': latest(str(STATIC_DIR / 'gsm_STATIC_COT_*.jsonl')),\n",
    "    'math':  latest(str(STATIC_DIR / 'math_STATIC_COT_*.jsonl')),\n",
    "    'math500': latest(str(STATIC_DIR / 'math500_STATIC_COT_*.jsonl')),\n",
    "  },\n",
    "  'dynamic': {\n",
    "    'math500_openmath_summary': latest(str(DYN_DIR / 'math500_openmath_summary_*.json')),\n",
    "    'math500_openmath_raw':     latest(str(DYN_DIR / 'math500_openmath_raw_*.json')),\n",
    "    'math500_mathpile_summary': latest(str(DYN_DIR / 'math500_mathpile_summary_*.json')),\n",
    "    'math500_mathpile_raw':     latest(str(DYN_DIR / 'math500_mathpile_raw_*.json')),\n",
    "  }\n",
    "}\n",
    "CFG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading + metrics\n",
    "def load_json_or_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    txt = path.read_text()\n",
    "    try:\n",
    "        data = json.loads(txt)\n",
    "        return data if isinstance(data, list) else [data]\n",
    "    except json.JSONDecodeError:\n",
    "        rows = []\n",
    "        for line in txt.splitlines():\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            rows.append(json.loads(line))\n",
    "        return rows\n",
    "\n",
    "def _clean_text(x: Optional[str]) -> Optional[str]:\n",
    "    if x is None: return None\n",
    "    s = str(x)\n",
    "    s = s.replace('\\\\boxed{', '').replace('\\\\dfrac{', '').replace('\\\\frac{', '')\n",
    "    s = s.replace('}{', '/').replace('}', '')\n",
    "    return s.strip()\n",
    "\n",
    "def _to_number(x: Optional[str]) -> Optional[Tuple[str, float]]:\n",
    "    if x is None: return None\n",
    "    xs = _clean_text(x) or ''\n",
    "    xs = xs.replace(',', '').replace('$', '').strip()\n",
    "    m = re.findall(r'[0-9]+(?:/[0-9]+)?', xs)\n",
    "    if not m: return (xs, math.nan)\n",
    "    tok = m[-1]\n",
    "    if '/' in tok:\n",
    "        a,b = tok.split('/')\n",
    "        try: val = float(a)/float(b)\n",
    "        except Exception: val = math.nan\n",
    "        return (tok, val)\n",
    "    try: return (tok, float(tok))\n",
    "    except Exception: return (tok, math.nan)\n",
    "\n",
    "def eq_numeric(a: Optional[str], b: Optional[str]) -> bool:\n",
    "    na = _to_number(a); nb = _to_number(b)\n",
    "    if na is None or nb is None: return False\n",
    "    sa, fa = na; sb, fb = nb\n",
    "    if sa == sb: return True\n",
    "    if (not math.isnan(fa)) and (not math.isnan(fb)):\n",
    "        return abs(fa - fb) <= 1e-6\n",
    "    return False\n",
    "\n",
    "def accuracy_from_rows(rows: List[Dict[str, Any]], pred_key: str, gold_key: str) -> float:\n",
    "    n = 0; c = 0\n",
    "    for r in rows:\n",
    "        p = r.get(pred_key); g = r.get(gold_key)\n",
    "        if p is None or g is None: continue\n",
    "        n += 1\n",
    "        c += 1 if eq_numeric(p, g) else 0\n",
    "    return (c / n) if n else float('nan')\n",
    "\n",
    "def to_frame(rows: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def show_ex(r: Dict[str, Any], keys: List[str]):\n",
    "    for k in keys:\n",
    "        v = r.get(k)\n",
    "        if isinstance(v, (dict, list)):\n",
    "            print(f'- {k}: [complex {type(v).__name__}]')\n",
    "        else:\n",
    "            s = str(v)\n",
    "            print(f'- {k}:', s[:400] + ('...' if len(s)>400 else ''))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 RQ1: Retrieval vs. No Retrieval\n",
    "- Compare accuracy between Baseline CoT and Standard RAG-CoT.\n",
    "- Provide qualitative examples where RAG helped vs. where it did not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_rq1(dataset: str):\n",
    "    cot_path    = CFG['cot'].get(dataset)\n",
    "    static_path = CFG['static'].get(dataset)\n",
    "    res = {}\n",
    "    if cot_path and cot_path.exists():\n",
    "        cot_rows = load_json_or_jsonl(cot_path)\n",
    "        res['cot_acc'] = accuracy_from_rows(cot_rows, 'prediction', 'answer')\n",
    "        res['cot_n'] = sum(1 for r in cot_rows if r.get('prediction') is not None and r.get('answer') is not None)\n",
    "    if static_path and static_path.exists():\n",
    "        s_rows = load_json_or_jsonl(static_path)\n",
    "        res['static_acc'] = accuracy_from_rows(s_rows, 'prediction', 'answer')\n",
    "        res['static_n'] = sum(1 for r in s_rows if r.get('prediction') is not None and r.get('answer') is not None)\n",
    "    return res\n",
    "\n",
    "rq1 = {d: eval_rq1(d) for d in ['gsm8k','math','math500']}\n",
    "pd.DataFrame(rq1).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_success_failure(static_path: Optional[Path], cot_path: Optional[Path], k: int = 2):\n",
    "    if not (static_path and static_path.exists() and cot_path and cot_path.exists()):\n",
    "        print('Static or CoT results missing; skipping paired examples.')\n",
    "        return\n",
    "    s_rows = load_json_or_jsonl(static_path)\n",
    "    c_rows = load_json_or_jsonl(cot_path)\n",
    "    n = min(len(s_rows), len(c_rows))\n",
    "    helped, hurt = [], []\n",
    "    for i in range(n):\n",
    "        s = s_rows[i]; c = c_rows[i]\n",
    "        oks = eq_numeric(s.get('prediction'), s.get('answer'))\n",
    "        okc = eq_numeric(c.get('prediction'), c.get('answer'))\n",
    "        if oks and not okc: helped.append((i,s,c))\n",
    "        if (not oks) and okc: hurt.append((i,s,c))\n",
    "    print(f'Found helped={len(helped)} hurt={len(hurt)}')\n",
    "    for tag, coll in [('RAG helped', helped[:k]), ('RAG hurt', hurt[:k])]:\n",
    "        print('\n===', tag, '===')\n",
    "        for i, s, c in coll:\n",
    "            show_ex({'question': s.get('question') or s.get('problem') or '',\n",
    "                     'gold': s.get('answer'),\n",
    "                     'static_pred': s.get('prediction'),\n",
    "                     'cot_pred': c.get('prediction')},\n",
    "                    ['question','gold','static_pred','cot_pred'])\n",
    "\n",
    "sample_success_failure(CFG['static'].get('math'), CFG['cot'].get('math'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 RQ2: Adaptive Retrieval Strategy\n",
    "- Retrieval frequency and when it triggers.\n",
    "- Accuracy vs. standard RAG-CoT.\n",
    "- Show decision-making (queries and injected context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dynamic(path: Optional[Path]) -> Optional[pd.DataFrame]:\n",
    "    if not (path and path.exists()): return None\n",
    "    rows = load_json_or_jsonl(path)\n",
    "    df = pd.DataFrame(rows)\n",
    "    if 'predicted_answer' in df.columns and 'answer' not in df.columns:\n",
    "        df = df.rename(columns={'predicted_answer':'prediction','ground_truth':'answer'})\n",
    "    return df\n",
    "\n",
    "def dyn_stats(df: Optional[pd.DataFrame]) -> Dict[str, Any]:\n",
    "    if df is None or df.empty: return {}\n",
    "    acc = accuracy_from_rows(df.to_dict('records'), 'prediction', 'answer')\n",
    "    rr  = float(df['retrieval_executed'].mean()) if 'retrieval_executed' in df else float('nan')\n",
    "    cnt = float(df.get('retrieval_count', pd.Series([0]*len(df))).mean())\n",
    "    return {'accuracy': acc, 'retrieval_rate': rr, 'avg_retrievals': cnt, 'n': len(df)}\n",
    "\n",
    "def keyword_corr(df: Optional[pd.DataFrame], keywords=('diagram','graph','matrix','geometry','probability','derivative','integral')):\n",
    "    if df is None or df.empty: return pd.DataFrame()\n",
    "    out = []\n",
    "    for kw in keywords:\n",
    "        mask = df['question'].str.contains(kw, case=False, na=False) if 'question' in df else pd.Series([False]*len(df))\n",
    "        rr = float(df.loc[mask, 'retrieval_executed'].mean()) if 'retrieval_executed' in df and mask.any() else float('nan')\n",
    "        out.append({'keyword': kw, 'retrieval_rate': rr, 'n': int(mask.sum())})\n",
    "    return pd.DataFrame(out).sort_values('retrieval_rate', ascending=False)\n",
    "\n",
    "dyn_openmath_sum = load_dynamic(CFG['dynamic']['math500_openmath_summary'])\n",
    "dyn_mathpile_sum = load_dynamic(CFG['dynamic']['math500_mathpile_summary'])\n",
    "dyn_openmath_raw = load_dynamic(CFG['dynamic']['math500_openmath_raw'])\n",
    "dyn_mathpile_raw = load_dynamic(CFG['dynamic']['math500_mathpile_raw'])\n",
    "\n",
    "stats_tbl = pd.DataFrame({\n",
    "  'math500_openmath_summary': dyn_stats(dyn_openmath_sum),\n",
    "  'math500_mathpile_summary': dyn_stats(dyn_mathpile_sum),\n",
    "  'math500_openmath_raw':     dyn_stats(dyn_openmath_raw),\n",
    "  'math500_mathpile_raw':     dyn_stats(dyn_mathpile_raw),\n",
    "}).T\n",
    "stats_tbl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Keyword correlation (OpenMath, summary):')\n",
    "display(keyword_corr(dyn_openmath_sum))\n",
    "print('Keyword correlation (MathPile, summary):')\n",
    "display(keyword_corr(dyn_mathpile_sum))\n",
    "\n",
    "def show_decisions(df: Optional[pd.DataFrame], k=3):\n",
    "    if df is None or df.empty: return\n",
    "    shown = 0\n",
    "    for _, r in df.iterrows():\n",
    "        if not r.get('retrieval_executed', False):\n",
    "            continue\n",
    "        print('— id', r.get('id'), '| retrieved:', r.get('retrieval_count'))\n",
    "        print('Q:', (r.get('question') or '')[:160])\n",
    "        print('queries:', (r.get('queries_used') or [])[:3])\n",
    "        inj = r.get('injections_made') or []\n",
    "        if inj and isinstance(inj[0], dict):\n",
    "            i0 = inj[0]\n",
    "            print('mode:', i0.get('injection_mode'), 'decision:', i0.get('decision'), 'conf:', i0.get('confidence'))\n",
    "            print('ctx preview:', (i0.get('ctx_injected_preview') or '')[:200])\n",
    "        print('pred:', r.get('prediction'), '| gold:', r.get('answer'))\n",
    "        print()\n",
    "        shown += 1\n",
    "        if shown >= k: break\n",
    "\n",
    "show_decisions(dyn_openmath_sum, k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 RQ3: Impact of Retrieval Quality\n",
    "- Case studies with retrieved docs and how they were incorporated.\n",
    "- Relevant doc → correct vs noisy doc → incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_case(df: Optional[pd.DataFrame], want_correct=True) -> Optional[Dict[str, Any]]:\n",
    "    if df is None or df.empty: return None\n",
    "    for _, r in df.iterrows():\n",
    "        ok = eq_numeric(r.get('prediction'), r.get('answer'))\n",
    "        if (ok and want_correct) or ((not ok) and (not want_correct)):\n",
    "            if r.get('retrieval_executed') and r.get('injections_made'):\n",
    "                return r.to_dict()\n",
    "    return None\n",
    "\n",
    "good = pick_case(dyn_openmath_sum, want_correct=True)\n",
    "bad  = pick_case(dyn_openmath_sum, want_correct=False)\n",
    "\n",
    "print('— Relevant → Correct example —')\n",
    "if good:\n",
    "    show_ex(good, ['id','question','prediction','answer'])\n",
    "    inj = good.get('injections_made')[0]\n",
    "    if isinstance(inj, dict):\n",
    "        show_ex(inj, ['query','ctx_injected_preview','decision','confidence'])\n",
    "else:\n",
    "    print('No example found.')\n",
    "\n",
    "print('\n— Noisy/Misleading → Incorrect example —')\n",
    "if bad:\n",
    "    show_ex(bad, ['id','question','prediction','answer'])\n",
    "    inj = bad.get('injections_made')[0]\n",
    "    if isinstance(inj, dict):\n",
    "        show_ex(inj, ['query','ctx_injected_preview','decision','confidence'])\n",
    "else:\n",
    "    print('No example found.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 RQ4: General vs. Domain-Specific Corpus\n",
    "Head-to-head accuracy comparison for MathPile vs OpenMathInstruct-2 (dynamic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_kb(df_open: Optional[pd.DataFrame], df_pile: Optional[pd.DataFrame]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for name, df in [('openmath', df_open), ('mathpile', df_pile)]:\n",
    "        if df is None or df.empty: continue\n",
    "        rows.append({\n",
    "            'kb': name,\n",
    "            'n': len(df),\n",
    "            'accuracy': accuracy_from_rows(df.to_dict('records'), 'prediction', 'answer'),\n",
    "            'retrieval_rate': float(df.get('retrieval_executed', pd.Series([np.nan]*len(df))).mean()),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print('Summary mode:')\n",
    "display(compare_kb(dyn_openmath_sum, dyn_mathpile_sum))\n",
    "print('Raw mode:')\n",
    "display(compare_kb(dyn_openmath_raw, dyn_mathpile_raw))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Findings\n",
    "Auto-generated summary from the above metrics. Edit for final write-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct(x):\n",
    "    return (f'{100*x:.1f}%' if (x==x) else 'na')\n",
    "\n",
    "lines = []\n",
    "for ds, vals in (rq1 or {}).items():\n",
    "    cot = vals.get('cot_acc'); stat = vals.get('static_acc')\n",
    "    if cot is None and stat is None: continue\n",
    "    lines.append(f\"{ds}: CoT={pct(cot)} | Static RAG-CoT={pct(stat)}\")\n",
    "print('RQ1 – Retrieval vs No-Retrieval:')\n",
    "for s in lines: print('-', s)\n",
    "\n",
    "print('\nRQ2 – Adaptive strategy (Math500):')\n",
    "display(stats_tbl if 'stats_tbl' in globals() else pd.DataFrame())\n",
    "\n",
    "print('\nRQ4 – Corpus comparison (Math500, dynamic):')\n",
    "display(compare_kb(dyn_openmath_sum, dyn_mathpile_sum))\n",
    "display(compare_kb(dyn_openmath_raw, dyn_mathpile_raw))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
