import csv
import os
from tqdm import tqdm
import numpy as np
from transformers import AutoTokenizer

# --- Configuration ---
# Point this to the clean TSV file generated by your chunking script
INPUT_TSV_FILE = "./thesis_datasets/openmathinstruct2/openmath_chunked_for_indexing_bge-m3.tsv"

# The tokenizer for the model you are using for indexing
TOKENIZER_NAME = "BAAI/bge-m3"

def analyze_tsv_token_lengths():
    print(f"Loading tokenizer: {TOKENIZER_NAME}")
    try:
        # BGE-M3 uses the underlying transformer's tokenizer
        tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)
    except Exception as e:
        print(f"Error loading tokenizer: {e}. Please ensure you have an internet connection.")
        return

    # Lists to store the token counts for each format
    question_lengths = []
    chunk_lengths = []
    combined_lengths = []

    print(f"Analyzing token lengths from: {INPUT_TSV_FILE}")

    # Get total line count for progress bar
    try:
        with open(INPUT_TSV_FILE, 'r', encoding='utf-8') as f:
            total_lines = sum(1 for line in f) - 1 # Subtract 1 for the header
    except FileNotFoundError:
        print(f"ERROR: Input file not found at {INPUT_TSV_FILE}. Please run the chunking script first.")
        return

    with open(INPUT_TSV_FILE, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f, delimiter='\t')
        
        for row in tqdm(reader, total=total_lines, desc="Analyzing Tokens"):
            problem = row.get("problem", "")
            chunk = row.get("solution_chunk", "")

            # 1. Analyze "Question only"
            if problem:
                question_lengths.append(len(tokenizer.encode(problem)))

            # 2. Analyze "Solution Chunk only"
            if chunk:
                chunk_lengths.append(len(tokenizer.encode(chunk)))

            # 3. Analyze "Combined" format
            if problem and chunk:
                combined_text = f"Question: {problem}\nAnswer Chunk: {chunk}"
                combined_lengths.append(len(tokenizer.encode(combined_text)))

    # --- Print Statistics ---
    print("\n" + "="*50)
    print("--- Token Length Analysis Results ---")
    
    q_lengths_np = np.array(question_lengths)
    c_lengths_np = np.array(chunk_lengths)
    j_lengths_np = np.array(combined_lengths)

    for name, data_np in [("Question Only", q_lengths_np), 
                          ("Solution Chunk Only", c_lengths_np), 
                          ("Combined (Question + Chunk)", j_lengths_np)]:
        if len(data_np) == 0:
            print(f"\nNo data found for '{name}'.")
            continue
        print(f"\nDistribution for '{name}':")
        print(f"  Count: {len(data_np):,}")
        print(f"  Min: {np.min(data_np):.0f}")
        print(f"  Max: {np.max(data_np):.0f}")
        print(f"  Mean: {np.mean(data_np):.2f}")
        print(f"  95th Percentile: {np.percentile(data_np, 95):.0f}")
        print(f"  99th Percentile: {np.percentile(data_np, 99):.0f}")
        print(f"  99.9th Percentile: {np.percentile(data_np, 99.9):.0f}")
    
    print("="*50)

if __name__ == "__main__":
    analyze_tsv_token_lengths()